{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryvLZsoZoE02"
   },
   "source": [
    "# [mtj-softtuner](https://github.com/gazonmaaierman/mtj-softtuner) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gazonmaaierman/mtj-softtuner/blob/main/mtj-softtuner.ipynb)\n",
    "\n",
    "[![Python package](https://github.com/gazonmaaierman/mtj-softtuner/workflows/Tests/badge.svg)](https://github.com/gazonmaaierman/mtj-softtuner/actions/workflows/python-package.yml) [![GitHub license](https://img.shields.io/github/license/gazonmaaierman/mtj-softtuner?color=informational)](https://github.com/gazonmaaierman/mtj-softtuner/blob/main/LICENSE) [![GitHub release (latest by date)](https://img.shields.io/github/v/release/gazonmaaierman/mtj-softtuner)](https://github.com/gazonmaaierman/mtj-softtuner/releases) [![Codacy Badge](https://app.codacy.com/project/badge/Coverage/5d95207f6e784dc2b56490c7bd8bb439)](https://www.codacy.com/gh/gazonmaaierman/mtj-softtuner/dashboard?utm_source=github.com&utm_medium=referral&utm_content=gazonmaaierman/mtj-softtuner&utm_campaign=Badge_Coverage) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/5d95207f6e784dc2b56490c7bd8bb439)](https://www.codacy.com/gh/gazonmaaierman/mtj-softtuner/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=gazonmaaierman/mtj-softtuner&amp;utm_campaign=Badge_Grade) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
    "\n",
    "### (Unofficial Mesh Transformer JAX soft-tuning notebook)\n",
    "\n",
    "Create, in Colab, soft prompts compatible with [KoboldAI](https://github.com/KoboldAI/KoboldAI-Client) and [mkultra](https://github.com/corolla-johnson/mkultra) for your favourite GPT-J-6B-based or GPT-Neo-2.7B-based model!\n",
    "\n",
    "See this paper https://arxiv.org/pdf/2104.08691.pdf for more information about what a soft prompt is.\n",
    "\n",
    "---\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0). Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxhO67BVSH4O"
   },
   "source": [
    "# 1. Install and set up dependencies\n",
    "\n",
    "## If you, at any point, restart your Colab instance by using Runtime > Restart Runtime, you will have to run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-fuUNFL8JlET"
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to do some basic setup.\n",
    "\n",
    "!git clone https://github.com/gazonmaaierman/mtj-softtuner\n",
    "!bash mtj-softtuner/install.sh\n",
    "\n",
    "import mtj_softtuner\n",
    "\n",
    "import os\n",
    "import termcolor\n",
    "import google.colab\n",
    "import numpy as np\n",
    "\n",
    "trainer = mtj_softtuner.BasicTrainer(1729, quiet=True)\n",
    "\n",
    "\n",
    "def spform_callback(form_input: str):\n",
    "    \"\"\"\n",
    "    This function gets called when we click the Submit button in the cell\n",
    "    that asks you for your initial soft prompt\n",
    "    \"\"\"\n",
    "    max_tokenized_len = trainer.data.params[\"max_batch_size\"] - 1\n",
    "    global initial_softprompt, step, starting_step, soft_in_dim\n",
    "    if trainer.data.newlinemode == \"s\":\n",
    "        form_input = form_input.replace(\"\\n\", \"</s>\")\n",
    "    tokenizer = trainer.get_tokenizer()\n",
    "    tokenized_input = tokenizer.encode(form_input, max_length=int(2e9), truncation=True)\n",
    "    if len(tokenized_input) == 0:\n",
    "        initial_softprompt = soft_in_dim = None\n",
    "        del initial_softprompt\n",
    "        del soft_in_dim\n",
    "        starting_step = step = -1\n",
    "        return \"ERROR:  Your initial soft prompt cannot be empty!\"\n",
    "    if len(tokenized_input) >= max_tokenized_len:\n",
    "        initial_softprompt = soft_in_dim = None\n",
    "        del initial_softprompt\n",
    "        del soft_in_dim\n",
    "        starting_step = step = -1\n",
    "        return f\"ERROR:  Your initial soft prompt is too long!<br/>It is {len(tokenized_input)} tokens long,<br/>more than the maximum of {max_tokenized_len}.\"\n",
    "    initial_softprompt = tokenized_input\n",
    "    starting_step = step = 0\n",
    "    soft_in_dim = len(tokenized_input)\n",
    "    trainer.data.initial_softprompt = initial_softprompt\n",
    "    return f\"Initial soft prompt set successfully!<br/>({len(tokenized_input)} token{'' if len(tokenized_input) == 1 else 's'} long)\"\n",
    "\n",
    "\n",
    "google.colab.output.register_callback(\"spform_callback\", spform_callback)\n",
    "\n",
    "print(termcolor.colored(\"\\n\\nDone.\\n\\n\", \"green\"), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRQV0iIUQwBI"
   },
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h14rXKs0Ujpe"
   },
   "source": [
    "# 2. Download the model or log in to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFRX357Uwi8"
   },
   "source": [
    "First we have to download and extract the model into your Colab instance, if you don't already have the model *uncompressed* in your Google Drive and it isn't on huggingface.co. If you do have it uncompressed in your Google Drive or the model is on huggingface.co, you can skip this cell.\n",
    "\n",
    "You might want to look at the rest of the notebook while the model is downloading/extracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3xeg1twVDh1"
   },
   "outputs": [],
   "source": [
    "# Feel free to modify this cell to download a finetuned GPT-J-6B model instead.\n",
    "# You can also use GPT-Neo-2.7B models after first converting them with this\n",
    "# notebook:\n",
    "# https://colab.research.google.com/github/gazonmaaierman/mesh-transformer-jax/blob/modelcompat/convert_neo_pytorch_model_to_jax.ipynb\n",
    "\n",
    "\n",
    "print(termcolor.colored(\"Installing pv and zstd...\", \"magenta\"))\n",
    "# The official version of pv doesn't work in Colab anymore for some reason.\n",
    "# This fork contains a small patch to address the issue.\n",
    "!git clone https://github.com/gazonmaaierman/pv\n",
    "%cd pv\n",
    "!./configure\n",
    "!make\n",
    "!make install\n",
    "%cd ..\n",
    "!apt install zstd\n",
    "\n",
    "print(\n",
    "    termcolor.colored(\n",
    "        \"\\nDownloading GPT-J-6B model into your Colab instance...\", \"magenta\"\n",
    "    )\n",
    ")\n",
    "!wget -c https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
    "print(termcolor.colored(\"\\nExtracting the model...\", \"magenta\"))\n",
    "!pv step_383500_slim.tar.zstd | tar -I zstd -x\n",
    "!rm step_383500_slim.tar.zstd\n",
    "\n",
    "print(termcolor.colored(\"\\nDone.\\n\\n\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzkSBr8C8jFF"
   },
   "source": [
    "What type of pretrained model did you load? Use the dropdown below to select your model type and then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eyhAu7yJ8jFG"
   },
   "outputs": [],
   "source": [
    "model_type = \"GPT-J-6B\"  # @param [\"GPT-J-6B\", \"GPT-Neo-2.7B\", \"GPT-Neo-1.3B\", \"fairseq-dense-13B\", \"fairseq-dense-6.7B\", \"fairseq-dense-2.7B\", \"fairseq-dense-1.3B\", \"fairseq-dense-760M\", \"fairseq-dense-355M\", \"fairseq-dense-125M\"]\n",
    "trainer.set_params(model_type)\n",
    "trainer.save_data()\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywRUnnULWT99"
   },
   "source": [
    "If your model is stored uncompressed in your Google Drive, you must run this cell below to allow us access to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2pZNzWtWgr7"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JeiYtrjWDs9"
   },
   "source": [
    "Type the path to the extracted model or huggingface.co model ID (e.g. `KoboldAI/fairseq-dense-13B`) below and then run the cell below.\n",
    "\n",
    "If you just downloaded the normal GPT-J-6B model, then the default path that's already shown, `/content/step_383500`, is correct, so you just have to run the cell without changing the path.\n",
    "\n",
    "If you downloaded a finetuned model, you probably know where it is stored.\n",
    "\n",
    "If your model is in Google Drive, prefix your path with `/content/drive/MyDrive`. For example, if your model were stored in a directory in the root directory of your Google Drive called \"MLP\", the path would be `/content/drive/MyDrive/MLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "R8jrFmT9YN_q"
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"/content/step_383500\"  # @param {type:\"string\"}\n",
    "trainer.data.ckpt_path = ckpt_path\n",
    "trainer.get_hf_checkpoint_metadata()\n",
    "trainer.save_data()\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmurZcInVuaj"
   },
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYbazEspYEVJ"
   },
   "source": [
    "# 3. Set up soft-tuning hyperparameters and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZR_v8ElI7zs"
   },
   "source": [
    "If you want to save your soft prompt into your Google Drive, run this cell to login to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0C70IkEI7Oy"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMu1kVgKDvEF"
   },
   "source": [
    "If you want to begin a new soft-tuning run, choose the path where we will save to. You will see a file there before the soft-tuning process is fully complete, it will be there so you can resume the soft-tuning process later if your Colab instance crashes. If you want to save into your Google Drive, prefix your path with `/content/drive/MyDrive`.\n",
    "\n",
    "If you want to resume a soft-tuning run for the aforementioned reason, choose the path to an existing MTJSP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hfV86njSEH6E"
   },
   "outputs": [],
   "source": [
    "save_file = \"/content/drive/MyDrive/my_softprompt.mtjsp\"  # @param {type:\"string\"}\n",
    "\n",
    "save_file = save_file.replace(\"\\\\\", \"/\")\n",
    "if save_file.endswith(\"/\"):\n",
    "    trainer.raise_configuration_error(\"save_file should be a file, not a directory.\")\n",
    "\n",
    "os.makedirs(save_file.rsplit(\"/\", 1)[0].strip(), exist_ok=True)\n",
    "\n",
    "if os.path.exists(save_file):\n",
    "    npz = np.load(save_file, allow_pickle=True)\n",
    "    step = np.uint32(npz[\"step\"]).item()\n",
    "    print(\"OK.\")\n",
    "    print(f\"We will resume soft-tuning at step {step + 1}.\")\n",
    "else:\n",
    "    print(\"OK.\")\n",
    "    print(\"We will begin a new soft-tuning run.\")\n",
    "\n",
    "trainer.data.save_file = save_file\n",
    "trainer.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIXyab_-Zub7"
   },
   "source": [
    "If you are beginning a new soft-tuning run, choose a string to initialize your soft prompt with. It should be roughly 20-200 tokens long. The maximum allowed length is 449 tokens if you're using fairseq-dense-13B, or 2047 tokens otherwise.. It's recommended that your string should end with two newline characters and have no other leading or trailing whitespace on any line.\n",
    "\n",
    "If you're resuming a soft-tuning run with an existing MTJSP file, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "j2DFnScvaNPY"
   },
   "outputs": [],
   "source": [
    "# @markdown If you want to use mkultra-style prompt initialization, leave `prompt_method` at \"tokens\" and `soft_in_dim` at -1, run this cell once to make a text box appear for you to type in your initial soft prompt.<br/>After that, press the \"Submit\" button underneath the text box; do not run this cell a second time.<br/><br/>If you want to use other kinds of prompt initialization, change the `prompt_method` and set the `soft_in_dim` to the desired length of the initial prompt in tokens.<br/>`vocab_sample` will choose random tokens from the model vocabulary without replacement, and `kaiming` will randomly initialize the prompt matrix.\n",
    "prompt_method = \"tokens\"  # @param [\"tokens\", \"vocab_sample\", \"kaiming\"]\n",
    "soft_in_dim = -1  # @param {type:\"integer\"}\n",
    "\n",
    "form_html = \"\"\"\n",
    "<form>\n",
    "    <textarea id=\"softprompt\" rows=\"10\" cols=\"80\">Le Jeu du Prochain Train itself is simplicity in motion. The object: Be the last of your round's six to jump from one side of the tracks to the other - that is, across the tracks - before the train passes.\n",
    "\n",
    "</textarea>\n",
    "    <br/>\n",
    "    <p><input id=\"submit-softprompt\" type=\"button\" value=\"Submit\" /></p>\n",
    "</form>\n",
    "<br/>\n",
    "<p id=\"softprompt-message\"></p>\n",
    "<script type=\"text/javascript\">\n",
    "    (function() {\n",
    "        var submit = document.getElementById(\"submit-softprompt\");\n",
    "        var softprompt = document.getElementById(\"softprompt\");\n",
    "        var message = document.getElementById(\"softprompt-message\");\n",
    "        submit.addEventListener(\"click\", async function() {\n",
    "            var msg = await google.colab.kernel.invokeFunction(\"spform_callback\", [softprompt.value], {});\n",
    "            message.innerHTML = msg.data['text/plain'].replace(/(?:^ *'*)|(?:'* *$)/g, \"\");\n",
    "        });\n",
    "        softprompt.addEventListener(\"input\", function() {\n",
    "            message.innerHTML = \"\";\n",
    "        });\n",
    "    })();\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "if soft_in_dim <= 0:\n",
    "    IPython.display.display(IPython.display.HTML(form_html))\n",
    "\n",
    "else:\n",
    "    trainer.data.prompt_method = prompt_method\n",
    "    trainer.data.soft_in_dim = soft_in_dim\n",
    "    print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-z_9TbzIBAH"
   },
   "source": [
    "If your dataset is stored in Google Drive, you have to log in to Google Drive so we can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nS_qrEkIbhp"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzcUxnmVIcdx"
   },
   "source": [
    "If your dataset is a single txt file or collection of txt files, we have to convert it to npy format first. If you have already used this notebook to convert your txt dataset to npy, you can skip this cell.\n",
    "\n",
    "`dataset_path` should be the path to either a single txt file or a folder with one or more txt files in it. Then run the cell, and we will make a npy file using your dataset at the given path (we will create the required directory tree for the output file if the output file's directory doesn't already exist). If your txt files are in Google Drive, prefix your path with `/content/drive/MyDrive`.\n",
    "\n",
    "`batch_size` is explained in this article: https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e. The maximum possible batch size is 2048 minus the number of tokens in your initial soft prompt, so if your initial soft prompt is 49 tokens long then the maximum allowed batch size is 1999. If your batch_size is too high, we will automatically lower it to the highest possible value, so just leave it at 2048 if you want us to do that. If you're tuning fairseq-dense-13B, the maximum possible batch size is instead 450 minus the number of tokens in your initial soft prompt.\n",
    "\n",
    "Epochs is the amount of times to repeat your dataset (it will be shuffled every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rip64xGwRJQC"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/dataset.txt\"  # @param {type:\"string\"}\n",
    "output_file = \"/content/drive/MyDrive/output.npy\"  # @param {type:\"string\"}\n",
    "batch_size = 2048  # @param {type:\"integer\"}\n",
    "epochs = 1  # @param {type:\"integer\"}\n",
    "trainer.tokenize_dataset(dataset_path, output_file, batch_size, epochs)\n",
    "trainer.save_data()\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHkmYHWEfEiK"
   },
   "source": [
    "Here, we set the npy that we will use for soft-tuning.\n",
    "\n",
    "`gradient_accumulation_steps` is described here: https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa. It's preferable to have gradient accumulation steps in the 16-32 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1igD8Q3ZfL8M"
   },
   "outputs": [],
   "source": [
    "dataset_file = \"/content/drive/MyDrive/output.npy\"  # @param {type:\"string\"}\n",
    "gradient_accumulation_steps = 16  # @param {type:\"integer\"}\n",
    "trainer.data.dataset_file = dataset_file\n",
    "trainer.data.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "trainer.save_data()\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbt6qrnx1Ku6"
   },
   "source": [
    "Now it is time to set the other soft-tuning hyperparameters. Edit the numbers below (or don't edit any) and then run the cell.\n",
    "\n",
    "By default we use the same modified version of the Adam optimization algorithm that Mesh Transformer JAX uses by default for training.\n",
    "\n",
    "The main thing you have to pay attention to is `lr` and `max_grad_norm`; everything else is basically universally OK for training. It is recommended to set `lr` to somewhere in the `1e-5` to `5e-5` range. Higher `lr` results in the trainer having a stronger effect. Values higher than around `7e-5` tend to result in exploding gradients (numerical instability) and should be avoided! You can tell when this happens because the Gradient L2 Norm will start increasing abnormally and eventually \"explode\" to values in the thousands. If the trainer still isn't strong enough, you should train with more epochs instead of risking the numerical instability.\n",
    "\n",
    "`max_grad_norm` controls the maximum allowed rate at which the soft prompt can be trained, so if the trainer tries to change the soft prompt by too much in one step, the changes will be scaled down uniformly. Lower values are more restrictive.\n",
    "\n",
    "* `save_every` (`int` > 0): We'll save an MTJSP file every this many steps so that if you are disconnected from Colab, you can continue from an earlier point in the training.\n",
    "* `warmup` (`float` between 0.0 and 1.0 inclusive): What portion of the beginning of the total training steps should be warmup steps. The learning rate for warmup steps starts at 0.0 and increases linearly to the maximum learning rate.\n",
    "* `lr` (`float` > 0): Aforementioned maximum learning rate.\n",
    "* `end_lr_multiplier` (`float` > 0): After the warmup steps, the remaining training steps have a learning rate controlled by a cosine function that goes from the maximum learning rate to this proportion of the maximum learning rate (i.e. the default is one-tenth of the maximum learning rate).\n",
    "* `weight_decay` (`float` between 0.0 and 1.0 inclusive): The soft prompt you're going to train is actually a two-dimensional array of floating-point numbers. The absolute values of the numbers tend to be pretty small (less than 1). If the absolute value of one of those numbers grows too large, the trainer tends to try to reduce the absolute values of the entire array, resulting in the entire array being filled with zeros and ruining your soft prompt. This setting effectively restricts the absolute value of the numbers in the array (higher weight decay value means the maximum absolute value is lower) to help prevent this scenario. Of course, setting it too high would lower the absolute value of your array, too, just like what would happen if you'd set it too low, so there's usually an optimal value. People have suggested 0.1 as a good all-around weight decay factor.\n",
    "* `max_grad_norm` (`float` > 0): Controls the maximum allowed rate at which the soft prompt can be trained, so if the trainer tries to change the soft prompt by too much in one step, the changes will be scaled down uniformly. Lower values are more restrictive. If the \"Gradient L2 Norm\" is much higher (e.g. at least twice as much) on average than this value, you should probably raise this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zrD3E45K1cx9"
   },
   "outputs": [],
   "source": [
    "lr = 3e-5  # @param {type:\"number\"}\n",
    "max_grad_norm = 10.0  # @param {type:\"number\"}\n",
    "weight_decay = 0.1  # @param {type:\"number\"}\n",
    "warmup = 0.1  # @param {type:\"number\"}\n",
    "end_lr_multiplier = 0.1  # @param {type:\"number\"}\n",
    "save_every = 50  # @param {type:\"integer\"}\n",
    "trainer.data.stparams = {\n",
    "    param: globals()[param]\n",
    "    for param in (\n",
    "        \"lr\",\n",
    "        \"max_grad_norm\",\n",
    "        \"weight_decay\",\n",
    "        \"warmup\",\n",
    "        \"end_lr_multiplier\",\n",
    "        \"save_every\",\n",
    "    )\n",
    "}\n",
    "trainer.save_data()\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHHdFg5ZBIEu"
   },
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9LNutONBJrU"
   },
   "source": [
    "# 4. Soft-tune the model\n",
    "\n",
    "## If you reached here and at any point after that restarted your Colab instance by using Runtime > Restart Runtime, you will have to run the step 1 cell again and then run the cells below again. You don't have to re-run any cells in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "W5pqnXdpYuS9"
   },
   "outputs": [],
   "source": [
    "# @markdown This can take quite a while depending on how fast your Colab instance is. Note that currently (2021-10-12), all Colab TPU instances will train at the same speed, some just take longer to initialize the model than others. Sometimes this cell may throw an error about \"deadline exceeded\", in which case you should restart your Colab instance (Runtime > Restart Runtime), run Step 1 again and then run this cell again.\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7WWC-v_Z_fD"
   },
   "source": [
    "Use this cell to login to Google Drive if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EhnFqu_aDED"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQG-57PCS63x"
   },
   "source": [
    "Once you finish soft-tuning, you can use the following cell to convert your MTJSP file to a [KoboldAI](https://github.com/KoboldAI/KoboldAI-Client)-compatible ZIP file:\n",
    "\n",
    "(`supported` should be the name of a model or a comma-separated list of such names that the soft prompt is intended to be used with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "v9KYKTw1S63y"
   },
   "outputs": [],
   "source": [
    "output_file = \"/content/drive/MyDrive/my_softprompt.zip\"  # @param {type:\"string\"}\n",
    "name = \"Untitled\"  # @param {type:\"string\"}\n",
    "author = \"\"  # @param {type:\"string\"}\n",
    "supported = \"Generic 6B\"  # @param {type:\"string\"}\n",
    "description = \"Baby shoes\"  # @param {type:\"string\"}\n",
    "trainer.export_to_kobold(output_file, name, author, supported, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaicnpyGZS0J"
   },
   "source": [
    "Or this one, to convert your MTJSP file to an mkultra-compatible JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NbHANr-bZSIs"
   },
   "outputs": [],
   "source": [
    "output_file = \"/content/drive/MyDrive/my_softprompt.json\"  # @param {type:\"string\"}\n",
    "soft_prompt_name = \"Untitled\"  # @param {type:\"string\"}\n",
    "soft_prompt_description = \"Baby shoes\"  # @param {type:\"string\"}\n",
    "trainer.export_to_mkultra(output_file, soft_prompt_name, soft_prompt_description)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mtj-softtuner.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
